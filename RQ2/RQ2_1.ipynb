{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 2-1\n",
    "Load the data from RQ1-2 results and perform filtering based on a threshold computed from the statistics on the results.\n",
    "### 1. Load the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 1196\n",
      "Number of responses: 1196\n",
      "Number of toxicity scores: 1196\n",
      "\n",
      "Number of prompts: 1196\n",
      "Number of responses: 1196\n",
      "Number of toxicity scores: 1196\n",
      "\n",
      "Number of prompts: 1196\n",
      "Number of responses: 1196\n",
      "Number of toxicity scores: 1196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the results back\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_evaluations(input_file):\n",
    "    prompts = []\n",
    "    all_responses = []\n",
    "    all_toxicity_scores = []\n",
    "\n",
    "    with open(input_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            prompts.append(data[\"prompt\"])\n",
    "            all_responses.append(data[\"responses\"])\n",
    "            all_toxicity_scores.append(data[\"toxicity_scores\"])\n",
    "\n",
    "    return prompts, all_responses, all_toxicity_scores\n",
    "\n",
    "bloom_7b1_result = '../RQ1/RQ1_2-results/bloom-7b1-toxic-template-1-evaluations.jsonl'\n",
    "llama_2b_result = '../RQ1/RQ1_2-results/llama-2-7b-hf-toxic-template-1-evaluations.jsonl'\n",
    "llama_3b_result = '../RQ1/RQ1_2-results/meta-llama-3-8b-toxic-template-1-evaluations.jsonl'\n",
    "\n",
    "prompts_all_models = []\n",
    "all_responses_all_models = []\n",
    "all_toxicity_scores_all_models = []\n",
    "\n",
    "for evaluation_result in [bloom_7b1_result, llama_2b_result, llama_3b_result]:\n",
    "    prompts, all_responses, all_toxicity_scores = load_evaluations(evaluation_result)\n",
    "    print(f\"Number of prompts: {len(prompts)}\")\n",
    "    print(f\"Number of responses: {len(all_responses)}\")\n",
    "    print(f\"Number of toxicity scores: {len(all_toxicity_scores)}\")\n",
    "    print()\n",
    "    prompts_all_models.append(prompts)\n",
    "    all_responses_all_models.append(all_responses)\n",
    "    all_toxicity_scores_all_models.append(all_toxicity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find the Thresold\n",
    "The threshold is computed by keeping the most toxic 25% outputs over all models. This not only keeps the most interesting problematic responses but also removes the meaningless charater responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for 25% most toxic: 0.8606906831264496\n"
     ]
    }
   ],
   "source": [
    "# Mix all the results and find the proper threshold\n",
    "all_toxicity_scores_combined = np.array([score['toxicity'] for all_scores in all_toxicity_scores_all_models for scores in all_scores for score in scores])\n",
    "most_toxic_percentage = 25\n",
    "percentile = np.percentile(all_toxicity_scores_combined, 100 - most_toxic_percentage)\n",
    "print(f\"Threshold for {most_toxic_percentage}% most toxic: {percentile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter the Evaluation Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [00:00, 148008.65it/s]\n",
      "229it [00:00, 179431.28it/s]\n",
      "464it [00:00, 182532.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the results\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_evaluations(output_file, prompts, all_responses, all_toxicity_scores):\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for i, toxicity_scores in tqdm(enumerate(all_toxicity_scores)):\n",
    "            # Convert all scores to JSON serializable types if necessary\n",
    "            serializable_scores = [\n",
    "                {key: float(value) for key, value in score.items()} for score in toxicity_scores\n",
    "            ]\n",
    "            data = {\n",
    "                \"prompt\": prompts[i],\n",
    "                \"responses\": all_responses[i],\n",
    "                \"toxicity_scores\": serializable_scores,\n",
    "            }\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "bloom_7b1_filtered = 'RQ2_1-results/bloom-7b1-toxic-template-1-evaluations-filtered.jsonl'\n",
    "llama_2b_filtered = 'RQ2_1-results/llama-2-7b-hf-toxic-template-1-evaluations-filtered.jsonl'\n",
    "llama_3b_filtered = 'RQ2_1-results/meta-llama-3-8b-toxic-template-1-evaluations-filtered.jsonl'\n",
    "\n",
    "for i, filtered_result in enumerate([bloom_7b1_filtered, llama_2b_filtered, llama_3b_filtered]):\n",
    "    prompts = prompts_all_models[i]\n",
    "    all_responses = all_responses_all_models[i]\n",
    "    all_toxicity_scores = all_toxicity_scores_all_models[i]\n",
    "\n",
    "    filtered_prompts = []\n",
    "    filtered_all_responses = []\n",
    "    filtered_all_toxicity_scores = []\n",
    "\n",
    "    for prompt, responses, scores in zip(prompts, all_responses, all_toxicity_scores) : # number of prompts which should be 1196\n",
    "        toxic_generation_scores = []\n",
    "        toxic_generation_responses = []\n",
    "        for score in scores: # each prompt can have multiple responses and multiple scores\n",
    "            if score['toxicity'] > percentile:\n",
    "                toxic_generation_scores.append(score)\n",
    "                toxic_generation_responses.append(responses[scores.index(score)])\n",
    "        if len(toxic_generation_scores): # meaning that this prompt has at least one toxic response\n",
    "            filtered_prompts.append(prompt) # so we keep this prompt\n",
    "            filtered_all_responses.append(toxic_generation_responses) # as well as its toxic responses\n",
    "            filtered_all_toxicity_scores.append(toxic_generation_scores)\n",
    "            \n",
    "    save_evaluations(filtered_result, filtered_prompts, filtered_all_responses, filtered_all_toxicity_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
